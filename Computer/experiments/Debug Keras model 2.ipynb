{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping1D, Cropping2D\n",
    "from keras.layers import Conv2D, Dropout\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from image_pipeline import Operation, Color, Sobel, Magnitude, Direction, \\\n",
    "  Threshold, Combinator, FindLinesSlidingWindows, Annotate, ImagePipeline\n",
    "import pickle\n",
    "\n",
    "MODEL_H5 = '/home/sku/model.h5'\n",
    "\n",
    "TARGET_WIDTH = 320\n",
    "TARGET_HEIGHT = 240\n",
    "TARGET_CROP = ((60, 20), (0, 0))\n",
    "\n",
    "# Look into Arduino code's car.h for SteerFeedMin_ and SteerFeedMax_\n",
    "STEER_MIN = 30\n",
    "STEER_MAX = 993\n",
    "\n",
    "STEER_FIELD_ID = 1\n",
    "SPEED_FIELD_ID = 2\n",
    "\n",
    "IMG_DIR = \"C:\\\\Users\\\\teguh\\\\Dropbox\\\\Projects\\\\Robotics\\\\Self-Driving-RC-Data\\\\recorded-2017-06-01.1\\\\recorded\"\n",
    "DATA_FILE = \"C:\\\\Users\\\\teguh\\\\Dropbox\\\\Projects\\\\Robotics\\\\Self-Driving-RC-Data\\\\recorded-2017-06-01.1\\\\recorded.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Keras Module\n",
    "\n",
    "First, let's see how the image looks like before it was passed to learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = load_model(MODEL_H5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import h5py\n",
    "\n",
    "def print_structure(weight_file_path):\n",
    "    \"\"\"\n",
    "    Prints out the structure of HDF5 file.\n",
    "\n",
    "    Args:\n",
    "      weight_file_path (str) : Path to the file to analyze\n",
    "    \"\"\"\n",
    "    f = h5py.File(weight_file_path)\n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))\n",
    "\n",
    "        if len(f.items())==0:\n",
    "            return \n",
    "\n",
    "        for layer, g in f.items():\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items():\n",
    "                print(\"      {}: {}\".format(key, value))\n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for p_name in g.keys():\n",
    "                param = g[p_name]\n",
    "                print(\"      {}: {}\".format(p_name, param.shape))\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'lambda_1', 'trainable': True, 'batch_input_shape': (None, 240, 320, 1), 'dtype': 'float32', 'function': ('ã\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00C\\x00\\x00\\x00s\\x08\\x00\\x00\\x00|\\x00d\\x01\\x1b\\x00S\\x00)\\x02Ng\\x00\\x00\\x00\\x00\\x00ào@©\\x00)\\x01Ú\\x01xr\\x01\\x00\\x00\\x00r\\x01\\x00\\x00\\x00ú3/home/jay/Self-Driving-RC/Computer/learner/learn.pyÚ\\x08<lambda>\\x9c\\x00\\x00\\x00ó\\x00\\x00\\x00\\x00', None, None), 'function_type': 'lambda', 'output_shape': None, 'output_shape_type': 'raw', 'arguments': {}}\n",
      "{'name': 'cropping2d_1', 'trainable': True, 'cropping': ((60, 20), (0, 0)), 'data_format': 'channels_last'}\n",
      "{'name': 'conv2d_1', 'trainable': True, 'filters': 24, 'kernel_size': (5, 5), 'strides': (2, 2), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(5, 5, 1, 24)\n",
      "Second:\n",
      "(24,)\n",
      "{'name': 'conv2d_2', 'trainable': True, 'filters': 36, 'kernel_size': (5, 5), 'strides': (2, 2), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(5, 5, 24, 36)\n",
      "Second:\n",
      "(36,)\n",
      "{'name': 'conv2d_3', 'trainable': True, 'filters': 48, 'kernel_size': (5, 5), 'strides': (2, 2), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(5, 5, 36, 48)\n",
      "Second:\n",
      "(48,)\n",
      "{'name': 'conv2d_4', 'trainable': True, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(3, 3, 48, 64)\n",
      "Second:\n",
      "(64,)\n",
      "{'name': 'conv2d_5', 'trainable': True, 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(3, 3, 64, 64)\n",
      "Second:\n",
      "(64,)\n",
      "{'name': 'flatten_1', 'trainable': True}\n",
      "{'name': 'dense_1', 'trainable': True, 'units': 100, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(27456, 100)\n",
      "Second:\n",
      "(100,)\n",
      "{'name': 'dense_2', 'trainable': True, 'units': 50, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(100, 50)\n",
      "Second:\n",
      "(50,)\n",
      "{'name': 'dense_3', 'trainable': True, 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(50, 10)\n",
      "Second:\n",
      "(10,)\n",
      "{'name': 'dense_4', 'trainable': True, 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "First:\n",
      "(10, 1)\n",
      "Second:\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    print (g)\n",
    "    if len(h) > 0:\n",
    "        print(\"First:\")\n",
    "        print(h[0].shape)\n",
    "        print(\"Second:\")\n",
    "        print(h[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c782697c1e8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# compute the gradient of the input picture wrt this loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# normalization trick: we normalize the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False,\n",
    "                           weights='imagenet')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "layer_name = 'block5_conv3'\n",
    "filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer\n",
    "\n",
    "# build a loss function that maximizes the activation\n",
    "# of the nth filter of the layer considered\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "# compute the gradient of the input picture wrt this loss\n",
    "grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "# normalization trick: we normalize the gradient\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "# this function returns the loss and grads given the input picture\n",
    "iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# we start from a gray image with some noise\n",
    "input_img_data = np.random.random((1, 3, img_width, img_height)) * 20 + 128.\n",
    "# run gradient ascent for 20 steps\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    input_img_data += grads_value * step\n",
    "    \n",
    "from scipy.misc import imsave\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "img = input_img_data[0]\n",
    "img = deprocess_image(img)\n",
    "imsave('%s_filter_%d.png' % (layer_name, filter_index), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-238ba110c54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0murllib2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pylab inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from urllib2 import urlopen\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "page = urlopen(\"http://datasets.flowingdata.com/ppg2008.csv\")\n",
    "nba = pd.read_csv(page, index_col=0)\n",
    "\n",
    "# Normalize data columns\n",
    "nba_norm = (nba - nba.mean()) / (nba.max() - nba.min())\n",
    "\n",
    "# Sort data according to Points, lowest to highest\n",
    "# This was just a design choice made by Yau\n",
    "# inplace=False (default) ->thanks SO user d1337\n",
    "nba_sort = nba_norm.sort('PTS', ascending=True)\n",
    "\n",
    "nba_sort['PTS'].head(10)\n",
    "\n",
    "# Plot it out\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(nba_sort, cmap=plt.cm.Blues, alpha=0.8)\n",
    "\n",
    "# Format\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 11)\n",
    "\n",
    "# turn off the frame\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_yticks(np.arange(nba_sort.shape[0]) + 0.5, minor=False)\n",
    "ax.set_xticks(np.arange(nba_sort.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "# Set the labels\n",
    "\n",
    "# label source:https://en.wikipedia.org/wiki/Basketball_statistics\n",
    "labels = [\n",
    "    'Games', 'Minutes', 'Points', 'Field goals made', 'Field goal attempts', 'Field goal percentage', 'Free throws made', 'Free throws attempts', 'Free throws percentage',\n",
    "    'Three-pointers made', 'Three-point attempt', 'Three-point percentage', 'Offensive rebounds', 'Defensive rebounds', 'Total rebounds', 'Assists', 'Steals', 'Blocks', 'Turnover', 'Personal foul']\n",
    "\n",
    "# note I could have used nba_sort.columns but made \"labels\" instead\n",
    "ax.set_xticklabels(labels, minor=False)\n",
    "ax.set_yticklabels(nba_sort.index, minor=False)\n",
    "\n",
    "# rotate the\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "# Turn off all the ticks\n",
    "ax = plt.gca()\n",
    "\n",
    "for t in ax.xaxis.get_major_ticks():\n",
    "    t.tick1On = False\n",
    "    t.tick2On = False\n",
    "for t in ax.yaxis.get_major_ticks():\n",
    "    t.tick1On = False\n",
    "    t.tick2On = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
